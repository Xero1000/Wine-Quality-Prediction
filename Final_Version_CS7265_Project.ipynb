{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing and Normalization"
      ],
      "metadata": {
        "id": "8oKa_mKJ-m7A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwLpIe_r-QI8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn import tree\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from random import sample\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from scipy.stats import norm\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "\n",
        "# Create dataframes for red wine quality dataset\n",
        "df_redWine = pd.read_csv(\"winequality-red.csv\")\n",
        "\n",
        "nancol = []\n",
        "no_ph = None \n",
        "\n",
        "i = 0\n",
        "while i < len(df_redWine):\n",
        "  if df_redWine.iat[i,11] == 3:\n",
        "    df_redWine.iat[i,11] = 4\n",
        "  elif df_redWine.iat[i,11] == 8:\n",
        "    df_redWine.iat[i,11] == 7\n",
        "  elif df_redWine.iat[i,11] == 6:\n",
        "    df_redWine.iat[i,11] = 5\n",
        "  i = i + 1\n",
        "\n",
        "df_wine = df_redWine.drop_duplicates().dropna() #duplicate rows and rows with null values are dropped\n",
        "df_wine_copy = df_wine.copy() # copy of the dataframe is made\n",
        "\n",
        "#min-max normalization\n",
        "for column in df_wine_copy.columns:\n",
        "  df_wine_copy[column] = (df_wine_copy[column] - df_wine_copy[column].min()) / (df_wine_copy[column].max() - df_wine_copy[column].min())\n",
        "\n",
        "df_wine_copy.loc[:, ['quality']] = df_wine[['quality']] #overwrite quality column with the original quality column values\n",
        "df_wine = df_wine_copy # set the original dataframe to the copy \n",
        "\n",
        "y = df_wine['quality']\n",
        "x = df_wine.drop(columns=['quality'])\n",
        "\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "x, y = oversample.fit_resample(x, y)\n",
        "\n",
        "x['quality'] = y\n",
        "\n",
        "y = df_wine['quality']\n",
        "x = df_wine.drop(columns=['quality'])\n",
        "\n",
        "# stratified K Fold for 10 splits\n",
        "skf = StratifiedKFold(n_splits=10)\n",
        "\n",
        "fold = 1 # The current fold\n",
        "\n",
        "# Split the data 10 times for 10 folds\n",
        "# Call each classifier method for each fold\n",
        "# Confusion Matrices and accuracies of each classifier are outputted for each fold\n",
        "for train_index, test_index in skf.split(x, y):\n",
        "    x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "    print('Fold: ', fold)\n",
        "    print()\n",
        "    customNB(x_train, x_test, y_train, y_test)\n",
        "    complementNB(x_train, x_test, y_train, y_test)\n",
        "    gaussianNB(x_train, x_test, y_train, y_test)\n",
        "    decTree(x_train, x_test, y_train, y_test)\n",
        "    svm(x_train, x_test, y_train, y_test)\n",
        "    logisticRegression(x_train, x_test, y_train, y_test)\n",
        "    kMeans(x_train, x_test, y_train, y_test)\n",
        "    kNN(x_train, x_test, y_train, y_test)\n",
        "    print('\\n---------------------\\n')\n",
        "    fold = fold + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gaussian and Complement Naive Bayes\n"
      ],
      "metadata": {
        "id": "_GepC-1P-VjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  def complementNB(x_train, x_test, y_train, y_test):\n",
        "  # Complement Naive Bayes\n",
        "  cnbClassifier = ComplementNB()\n",
        "  cnbClassifier.fit(x_train, y_train)\n",
        "\n",
        "  prediction = cnbClassifier.predict(x_test)\n",
        "  print(f\"Complement Naive Bayes Accuracy : {accuracy_score(y_test, prediction) * 100} % \\n\\n\")\n",
        "\n",
        "def gaussianNB(x_train, x_test, y_train, y_test):\n",
        "  # Gaussian Naive Bayes\n",
        "  gnbClassifier = GaussianNB()\n",
        "  gnbClassifier.fit(x_train, y_train)\n",
        "\n",
        "  gaussianPrediction = gnbClassifier.predict(x_test)\n",
        "  print(f\"Gaussian Naive Bayes Accuracy : {accuracy_score(y_test, gaussianPrediction) * 100} % \\n\\n\")"
      ],
      "metadata": {
        "id": "nNA1v3sa-VFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree\n"
      ],
      "metadata": {
        "id": "gUQLPudGGjVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decTree(x_train, x_test, y_train, y_test):\n",
        "  # Decision Tree \n",
        "  clf = tree.DecisionTreeClassifier()\n",
        "  clf = clf.fit(x_train, y_train)\n",
        "  pred = clf.predict(x_test)\n",
        "  print(f\"Decision Tree Accuracy : {accuracy_score(y_test, pred) * 100} % \\n\\n\")\n",
        "\n",
        "  # Decision Tree with ensemble learning\n",
        "  bagClassifier = BaggingClassifier(base_estimator=clf, n_estimators=20, max_samples=.5)\n",
        "  bagClassifier.fit(x_train, y_train)\n",
        "  print(f\"Decision Tree Bagging Classifier Accuracy: {bagClassifier.score(x_test, y_test) * 100} % \\n\\n\")"
      ],
      "metadata": {
        "id": "t-Uh_LqPGnmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine"
      ],
      "metadata": {
        "id": "HcYDCKi3G6vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def buildSVM(Xtrain, Xtest, ytrain, ytest, message):\n",
        "    classifier = SVC(random_state=5, kernel=\"rbf\", C=10, gamma=0.1)\n",
        "    trained_model = classifier.fit(Xtrain, ytrain)\n",
        "    y_pred = classifier.predict(Xtest)\n",
        "    print(message, accuracy_score(ytest, y_pred) * 100, \"\\n\\n\")"
      ],
      "metadata": {
        "id": "_jbbdRieHnoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svm(x_train, x_test, y_train, y_test):  \n",
        "  global nancol\n",
        "  global no_ph\n",
        "  tuned_parameters = [\n",
        "      {\n",
        "          'kernel': ['rbf'],\n",
        "          'gamma': [1e-1, 1e-2, 1e-3, 1e-4],\n",
        "          'C': [1, 10, 100, 1000]\n",
        "      },\n",
        "      {\n",
        "          'kernel': ['linear'],\n",
        "          'gamma': [1e-1, 1e-2, 1e-3, 1e-4],\n",
        "          'C': [1, 10, 100, 1000]\n",
        "      },\n",
        "      {\n",
        "          'kernel': ['sigmoid'],\n",
        "          'gamma': [1e-1, 1e-2, 1e-3, 1e-4],\n",
        "          'C': [1, 10, 100, 1000]\n",
        "      },\n",
        "      # {\n",
        "      #     'kernel': ['poly'],\n",
        "      #     'gamma': [1e-1, 1e-2, 1e-3, 1e-4],\n",
        "      #     'C': [1, 10, 100, 1000],\n",
        "      #     'degree': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "      # }\n",
        "  ]\n",
        "\n",
        "  # Support Vector Machine\n",
        "  sc = StandardScaler()\n",
        "  X_train = sc.fit_transform(x_train)\n",
        "  X_test = sc.transform(x_test)\n",
        "  classifier = SVC(random_state=5, kernel=\"rbf\", C=10, gamma=0.1)\n",
        "  trained_model = classifier.fit(x_train, y_train)\n",
        "  y_pred = classifier.predict(x_test)\n",
        "  print(f\"Support Vector Machine Accuracy : {accuracy_score(y_test, y_pred) * 100} % \\n\\n\")\n",
        "\n",
        "  erased = len(X_train) * 33 // 100\n",
        "  not_erased = len(X_train) - erased\n",
        "  nancol = sample(range(len(X_train)), erased) #παίρνω τυχαίο δείγμα του 33% των rows\n",
        "  for index in nancol:\n",
        "      X_train[index][8] = None\n",
        "\n",
        "  no_ph = np.delete(X_train,[8], axis=1) \n",
        "  no_ph_test = np.delete(X_test,[8], axis=1)\n",
        "  buildSVM(no_ph,no_ph_test,y_train,y_test, \"SVM Accuracy (No pH): \")\n",
        "\n",
        "  median = 0\n",
        "  for index in range(len(X_train)):\n",
        "      if index not in nancol:\n",
        "          median += X_train[index][8]\n",
        "\n",
        "  X_median = np.array(X_train)\n",
        "  median = median / not_erased\n",
        "  for index in range(len(X_median)):\n",
        "      if index in nancol:\n",
        "        X_median[index][8] = median\n",
        "\n",
        "  buildSVM(X_median, X_test, y_train, y_test, \"SVM Accuracy (pH with median values): \")"
      ],
      "metadata": {
        "id": "YWmg8D6xG876"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "mNeaj_yBcrkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logisticRegression(x_train, x_test, y_train, y_test):  \n",
        "  global nancol\n",
        "  y_ph = x_train.loc[:, ['pH']] \n",
        "  \n",
        "  sc = StandardScaler()\n",
        "  xg_train = []\n",
        "  yg_train = []\n",
        "  xg_test = []\n",
        "  yg_test = []\n",
        "  X_train2 = x_train.values\n",
        "  np.set_printoptions(suppress=True)\n",
        "\n",
        "  X_train2[:,0] = X_train2[:,0]*10\n",
        "  X_train2[:,1] = X_train2[:,1]*1000\n",
        "  X_train2[:,2] = X_train2[:,2]*100\n",
        "  X_train2[:,3] = X_train2[:,3]*10\n",
        "  X_train2[:,4] = X_train2[:,4]*1000\n",
        "  X_train2[:,7] = X_train2[:,7]*100000\n",
        "  X_train2[:,8] = X_train2[:,8]*100\n",
        "  X_train2[:,9] = X_train2[:,9]*100\n",
        "  X_train2[:,10] = X_train2[:,10]*10\n",
        "  X_train2.astype(int)\n",
        "\n",
        "  no_ph = np.delete(X_train2,[8], axis=1) \n",
        "  for i in range(len(X_train2)):\n",
        "      if i not in nancol:\n",
        "          xg_train.append(no_ph[i])\n",
        "          yg_train.append(y_train.iat[i])\n",
        "      if i in nancol:\n",
        "          xg_test.append(no_ph[i])\n",
        "          yg_test.append(y_train.iat[i])\n",
        "\n",
        "  logistic_regression = LogisticRegression(max_iter=120000)\n",
        "  logistic_regression.fit(xg_train, yg_train)\n",
        "  y_pred = logistic_regression.predict(xg_test)\n",
        "\n",
        "  j=0\n",
        "  for i in range(len(X_train2)):\n",
        "      if i in nancol:\n",
        "        X_train2[i][8] = y_pred[j]\n",
        "        j += 1\n",
        "\n",
        "  X_train2[:,0] = X_train2[:,0]/10\n",
        "  X_train2[:,1] = X_train2[:,1]/1000\n",
        "  X_train2[:,2] = X_train2[:,2]/100\n",
        "  X_train2[:,3] = X_train2[:,3]/10\n",
        "  X_train2[:,4] = X_train2[:,4]/1000\n",
        "  X_train2[:,7] = X_train2[:,7]/100000\n",
        "  X_train2[:,8] = X_train2[:,8]/100\n",
        "  X_train2[:,9] = X_train2[:,9]/100\n",
        "  X_train2[:,10] = X_train2[:,10]/10\n",
        "  X_train2 = sc.fit_transform(X_train2)\n",
        "\n",
        "  buildSVM(X_train2, x_test, y_train, y_test, \"Logistic Regression Accuracy: \")"
      ],
      "metadata": {
        "id": "ZQwUG7vycvy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Means"
      ],
      "metadata": {
        "id": "wrFWTG7xd5_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kMeans(x_train, x_test, y_train, y_test):  \n",
        "  global no_ph\n",
        "  no_ph1 = x_train.drop('pH', axis=1)\n",
        "  X_kmeans = x_train.values.tolist()\n",
        "  X_kplot = no_ph1.values.tolist()\n",
        "  \n",
        "  pH=8\n",
        "  for index in nancol:\n",
        "      X_kmeans[index][pH] = None\n",
        "\n",
        "  wcss = []\n",
        "  for i in range(1, 11):\n",
        "      kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "      kmeans.fit(X_kplot)\n",
        "      wcss.append(kmeans.inertia_)\n",
        "  plt.plot(range(1, 11), wcss)\n",
        "  plt.title('Elbow Method')\n",
        "  plt.xlabel('Number of clusters')\n",
        "  plt.ylabel('WCSS')\n",
        "  plt.savefig(\"kmeans.png\")\n",
        "  plt.show()\n",
        "  #Αυτα ειναι για να δειξουμε γιατι 4\n",
        "\n",
        "\n",
        "  kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "  centers = kmeans.fit(X_kplot).cluster_centers_\n",
        "\n",
        "  clusters = []\n",
        "  for i in range(len(centers)):\n",
        "      clusters.append([])\n",
        "  for i in range(len(X_kplot)):\n",
        "      dist = []\n",
        "      for j in range(len(centers)):\n",
        "          dist.append([np.linalg.norm(X_kplot[i]-centers[j]),j])\n",
        "      dist.sort()\n",
        "      clusters[dist[0][1]].append(i)\n",
        "\n",
        "  median = []\n",
        "  for i in range(len(clusters)):\n",
        "      median.append(0)\n",
        "\n",
        "  clust_num = 0\n",
        "  #print(len(clusters))\n",
        "  for cluster in clusters:\n",
        "      not_erased=0\n",
        "      for index in cluster:\n",
        "          if X_kmeans[index][pH] is not None:\n",
        "              median[clust_num] += X_kmeans[index][pH]\n",
        "              not_erased+=1\n",
        "      if not_erased != 0:\n",
        "          median[clust_num] /=not_erased\n",
        "      for index in cluster:\n",
        "          if X_kmeans[index][pH] is None:\n",
        "              X_kmeans[index][pH] = median[clust_num]\n",
        "      clust_num+=1\n",
        "\n",
        "  sc = StandardScaler()\n",
        "  X_noph = sc.fit_transform(no_ph)\n",
        "  X_kmeans = sc.fit_transform(X_kmeans)\n",
        "  print()\n",
        "  buildSVM(X_kmeans, x_test, y_train, y_test, \"K-Means Accuracy: \")"
      ],
      "metadata": {
        "id": "JYuLEtPrd8xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbors\n"
      ],
      "metadata": {
        "id": "Q22J_f-rruJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model(model, k, x_train, x_test, y_train, y_test):\n",
        "  model.fit(x_train, y_train)\n",
        "  preds = model.predict(x_test)\n",
        "  print(\"K = \", k, \" Accuracy Score is: \", accuracy_score(y_test,preds) *100)\n",
        "\n",
        "def kNN(x_train, x_test, y_train, y_test):\n",
        "  kvalues = [3, 5, 15, 18, 20, 25]\n",
        "  print(\"K-Nearest Neighbors Accuracies\")\n",
        "  for k in kvalues:\n",
        "    model = KNeighborsClassifier(n_neighbors= k)\n",
        "    run_model(model, k, x_train, x_test, y_train, y_test)\n"
      ],
      "metadata": {
        "id": "EJvl81w5rw_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Naive Bayes"
      ],
      "metadata": {
        "id": "Ku7gWkuv-cNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def customNB(x_train, x_test, y_train, y_test):\n",
        "  quantity4 = 0\n",
        "  quantity5 = 0\n",
        "  quantity7 = 0\n",
        "\n",
        "  quality4means = []\n",
        "  quality5means = []\n",
        "  quality7means = []\n",
        "\n",
        "  quality4stds = []\n",
        "  quality5stds = []\n",
        "  quality7stds = []\n",
        "\n",
        "  i = 0\n",
        "  while i < len(y_train):\n",
        "    if y_train.iat[i] == 4:\n",
        "      quantity4 = quantity4 + 1\n",
        "    elif y_train.iat[i] == 5:\n",
        "      quantity5 = quantity5 + 1\n",
        "    elif y_train.iat[i] == 7:\n",
        "      quantity7 = quantity7 + 1\n",
        "    i = i + 1\n",
        "\n",
        "  priorProb4 = quantity4 / len(y_train)\n",
        "  priorProb5 = quantity5 / len(y_train)\n",
        "  priorProb7 = quantity7 / len(y_train)\n",
        "\n",
        "  c = 0\n",
        "  while c <= 10:\n",
        "    r = 0\n",
        "    mean4 = 0\n",
        "    mean5 = 0\n",
        "    mean7 = 0\n",
        "    \n",
        "    while r < len(x_train):\n",
        "      if y_train.iat[r] == 4:\n",
        "        mean4 = mean4 + x_train.iat[r, c]\n",
        "      elif y_train.iat[r] == 5:\n",
        "        mean5 = mean5 + x_train.iat[r, c]\n",
        "      elif y_train.iat[r] == 7:\n",
        "        mean7 = mean7 + x_train.iat[r, c]\n",
        "      r = r + 1\n",
        "\n",
        "    if quantity4 != 0:\n",
        "      mean4 = mean4 / quantity4\n",
        "      quality4means.append(mean4)\n",
        "    else:\n",
        "      quality4means.append(0)\n",
        "    if quantity5 != 0:\n",
        "      mean5 = mean5 / quantity5\n",
        "      quality5means.append(mean5)\n",
        "    else:\n",
        "      quality5means.append(0)\n",
        "    if quantity7 != 0:\n",
        "      mean7 = mean7 / quantity7\n",
        "      quality7means.append(mean7)\n",
        "    else:\n",
        "      quality7means.append(0)\n",
        "    c = c + 1\n",
        "  \n",
        "  c = 0\n",
        "  while c <= 10:\n",
        "    r = 0\n",
        "    std4 = 0\n",
        "    std5 = 0\n",
        "    std7 = 0\n",
        "    while r < len(x_train):\n",
        "      if y_train.iat[r] == 4:\n",
        "        std4 = std4 + ((x_train.iat[r, c] - quality4means[c])**2)\n",
        "      elif y_train.iat[r] == 5:\n",
        "        std5 = std5 + ((x_train.iat[r, c] - quality5means[c])**2)\n",
        "      elif y_train.iat[r] == 7:\n",
        "        std7 = std7 + ((x_train.iat[r, c] - quality7means[c])**2)\n",
        "      r = r + 1\n",
        "\n",
        "    if quantity4 != 0:\n",
        "      std4 = math.sqrt(std4 / quantity4)\n",
        "      quality4stds.append(std4)\n",
        "    else:\n",
        "      quality4stds.append(0)\n",
        "    if quantity5 != 0:\n",
        "      std5 = math.sqrt(std5 / quantity5)\n",
        "      quality5stds.append(std5)\n",
        "    else:\n",
        "      quality5stds.append(0)\n",
        "    if quantity7 != 0:\n",
        "      std7 = math.sqrt(std7 / quantity7)\n",
        "      quality7stds.append(std7)\n",
        "    else:\n",
        "      quality7stds.append(0)\n",
        "    c = c + 1\n",
        "\n",
        "  predictedClasses = []\n",
        "\n",
        "  r = 0\n",
        "  while r < len(x_test):\n",
        "    c = 0\n",
        "    prob4 = 1\n",
        "    prob5 = 1\n",
        "    prob7 = 1\n",
        "\n",
        "    while c <= 10:\n",
        "      prob4 = prob4 * norm.pdf(x_test.iat[r, c], quality4means[c], quality4stds[c])\n",
        "      prob5 = prob5 * norm.pdf(x_test.iat[r, c], quality5means[c], quality5stds[c])\n",
        "      prob7 = prob7 * norm.pdf(x_test.iat[r, c], quality7means[c], quality7stds[c])\n",
        "      c = c + 1\n",
        "    \n",
        "    prob4 = prob4 * priorProb4\n",
        "    prob5 = prob5 * priorProb5\n",
        "    prob7 = prob7 * priorProb7\n",
        "\n",
        "    probList = [prob4, prob5, prob7]\n",
        "    maxProb = np.nanmax(probList)\n",
        "    maxProbIndex = probList.index(maxProb)\n",
        "    \n",
        "    if maxProbIndex == 0:\n",
        "      predictedClasses.append(4)\n",
        "    elif maxProbIndex == 1:\n",
        "      predictedClasses.append(5)\n",
        "    elif maxProbIndex == 2:\n",
        "      predictedClasses.append(7)\n",
        "    r = r + 1\n",
        "\n",
        "  r = 0\n",
        "  correct = 0\n",
        "  while r < len(x_test):\n",
        "    if y_test.iat[r] == predictedClasses[r]:\n",
        "      correct = correct + 1\n",
        "    r = r + 1\n",
        "  accuracy = correct / len(x_test)\n",
        "  print(\"Custom Naive Bayes Accuracy : \", accuracy * 100,\"% \\n\\n\")"
      ],
      "metadata": {
        "id": "v2y2Uv3y-dtV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}